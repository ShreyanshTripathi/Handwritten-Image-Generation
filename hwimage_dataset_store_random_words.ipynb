{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "This notebook contains code used for generating handwriting images dataset using simple handwriting fonts available.<br>\n",
    "Author: Shreyansh Tripathi<br>\n",
    "email: [shreyanshtripathi03@gmail.com](Hidden_landing_URL)\n",
    "\n",
    "The first version of this notebook is for creating a dummy dataset which can be extended easily to create bigger and advanced datasets using fonts from different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract words\n",
    "English language has roughly 250000 words. The dataset generation requires words form a dictionary. Here, words are taken from words corpus provided by nltk. In this dummy dataset we are taking random 1000 words from the corpus and producing its images. 1000 words are taken and saved in a list.<br>\n",
    "This can be extended to include most frequent words in a database in further implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[['suburbed']\n",
      " ['levulin']\n",
      " ['preprint']\n",
      " ['interregal']\n",
      " ['serophthisis']\n",
      " ['puncture']\n",
      " ['Ottawa']\n",
      " ['ostensible']\n",
      " ['geognosist']\n",
      " ['conversationable']\n",
      " ['querulosity']\n",
      " ['proxyship']\n",
      " ['nephrocyte']\n",
      " ['variability']\n",
      " ['undevelopable']\n",
      " ['unusurping']\n",
      " ['sphenoethmoidal']\n",
      " ['depressingly']\n",
      " ['zygostyle']\n",
      " ['treating']\n",
      " ['bathybian']\n",
      " ['dawkin']\n",
      " ['Sarcoptes']\n",
      " ['ceramic']\n",
      " ['midautumn']\n",
      " ['siderealize']\n",
      " ['laic']\n",
      " ['eyewear']\n",
      " ['elaeagnaceous']\n",
      " ['Halosphaera']\n",
      " ['ponto']\n",
      " ['ethnocentric']\n",
      " ['shaped']\n",
      " ['theosophistical']\n",
      " ['Dyotheletical']\n",
      " ['redistrainer']\n",
      " ['Trachoma']\n",
      " ['genialness']\n",
      " ['amentia']\n",
      " ['circumcise']\n",
      " ['idylist']\n",
      " ['eglantine']\n",
      " ['neurochemistry']\n",
      " ['columned']\n",
      " ['overrace']\n",
      " ['slumberousness']\n",
      " ['Anti']\n",
      " ['hypocone']\n",
      " ['Archibuteo']\n",
      " ['impressment']\n",
      " ['masterwork']\n",
      " ['bavary']\n",
      " ['pycnodont']\n",
      " ['translocalization']\n",
      " ['striga']\n",
      " ['remint']\n",
      " ['twitlark']\n",
      " ['bemusement']\n",
      " ['bolograph']\n",
      " ['hydromedusa']\n",
      " ['Gregarinida']\n",
      " ['enforceability']\n",
      " ['Gerbillinae']\n",
      " ['familiarism']\n",
      " ['premierjus']\n",
      " ['sapan']\n",
      " ['decemcostate']\n",
      " ['do']\n",
      " ['ramshackled']\n",
      " ['Moslemic']\n",
      " ['hiodont']\n",
      " ['beeishness']\n",
      " ['outdo']\n",
      " ['umbellulate']\n",
      " ['sinfulness']\n",
      " ['lipotype']\n",
      " ['scull']\n",
      " ['acology']\n",
      " ['gargoyle']\n",
      " ['rentee']\n",
      " ['nemertine']\n",
      " ['hypotension']\n",
      " ['admonitorial']\n",
      " ['Mariamman']\n",
      " ['unbasedness']\n",
      " ['adulterate']\n",
      " ['windowful']\n",
      " ['anacampsis']\n",
      " ['theory']\n",
      " ['concertize']\n",
      " ['institutionalist']\n",
      " ['tricosyl']\n",
      " ['stupp']\n",
      " ['ferroinclave']\n",
      " ['prestimulation']\n",
      " ['autoinduction']\n",
      " ['pupilate']\n",
      " ['substantivize']\n",
      " ['assistant']\n",
      " ['Polymnestor']\n",
      " ['meningomalacia']\n",
      " ['suggestibly']\n",
      " ['Sterope']\n",
      " ['unartfully']\n",
      " ['flit']\n",
      " ['girdlelike']\n",
      " ['preinvite']\n",
      " ['Logria']\n",
      " ['incompensation']\n",
      " ['gandul']\n",
      " ['strainer']\n",
      " ['antigonococcic']\n",
      " ['philippus']\n",
      " ['intrapsychical']\n",
      " ['asshead']\n",
      " ['frutify']\n",
      " ['pseudocele']\n",
      " ['platynite']\n",
      " ['sequestrum']\n",
      " ['demographically']\n",
      " ['paraphasia']\n",
      " ['pressurage']\n",
      " ['pentander']\n",
      " ['sea']\n",
      " ['allophylian']\n",
      " ['bromocamphor']\n",
      " ['Fingall']\n",
      " ['polytheistic']\n",
      " ['temenos']\n",
      " ['nicotinize']\n",
      " ['lithosol']\n",
      " ['conglutinant']\n",
      " ['anthranoyl']\n",
      " ['hydrokinetic']\n",
      " ['coalless']\n",
      " ['mokum']\n",
      " ['unweddedly']\n",
      " ['unmissionary']\n",
      " ['larcenic']\n",
      " ['parishionership']\n",
      " ['Dawsoniaceae']\n",
      " ['unconcerned']\n",
      " ['easting']\n",
      " ['volipresence']\n",
      " ['Ovambo']\n",
      " ['hourglass']\n",
      " ['unleashed']\n",
      " ['tambourinade']\n",
      " ['broad']\n",
      " ['tunbelly']\n",
      " ['porogamous']\n",
      " ['vapulate']\n",
      " ['succentor']\n",
      " ['kulang']\n",
      " ['Zionist']\n",
      " ['tithable']\n",
      " ['tauricornous']\n",
      " ['alleviate']\n",
      " ['Hibernize']\n",
      " ['cuprodescloizite']\n",
      " ['calcigerous']\n",
      " ['vindicatory']\n",
      " ['orbiculated']\n",
      " ['mistime']\n",
      " ['fabular']\n",
      " ['rota']\n",
      " ['semiresinous']\n",
      " ['Irish']\n",
      " ['ricinelaidic']\n",
      " ['platycercine']\n",
      " ['bisinuation']\n",
      " ['Turkman']\n",
      " ['metapleure']\n",
      " ['durax']\n",
      " ['hereness']\n",
      " ['mistaker']\n",
      " ['animability']\n",
      " ['sudden']\n",
      " ['televisionary']\n",
      " ['coreplastic']\n",
      " ['decalcify']\n",
      " ['san']\n",
      " ['koltunnor']\n",
      " ['meteoritics']\n",
      " ['jell']\n",
      " ['perkingly']\n",
      " ['unhalved']\n",
      " ['multisensual']\n",
      " ['murgavi']\n",
      " ['santims']\n",
      " ['Mytiliaspis']\n",
      " ['interjaculatory']\n",
      " ['panopticon']\n",
      " ['interscience']\n",
      " ['cerebrospinant']\n",
      " ['nuditarian']\n",
      " ['steg']\n",
      " ['undauntedly']\n",
      " ['spool']\n",
      " ['nonverbal']\n",
      " ['interlocutory']\n",
      " ['lexicality']\n",
      " ['filamentoid']\n",
      " ['writative']\n",
      " ['alan']\n",
      " ['tilmus']\n",
      " ['Pupivora']\n",
      " ['keepership']\n",
      " ['bonesetting']\n",
      " ['cymotrichy']\n",
      " ['Ophrys']\n",
      " ['Rickettsia']\n",
      " ['unforceable']\n",
      " ['sitten']\n",
      " ['cripes']\n",
      " ['variation']\n",
      " ['Scandian']\n",
      " ['lithophyllous']\n",
      " ['overlittle']\n",
      " ['well']\n",
      " ['irritable']\n",
      " ['aurelian']\n",
      " ['intrafactory']\n",
      " ['akey']\n",
      " ['zymotechnics']\n",
      " ['tormodont']\n",
      " ['osteogenic']\n",
      " ['Sicambrian']\n",
      " ['denehole']\n",
      " ['orthocephalic']\n",
      " ['phosphorous']\n",
      " ['unconcealing']\n",
      " ['Valmy']\n",
      " ['usurious']\n",
      " ['rotaliform']\n",
      " ['somniloquize']\n",
      " ['repeal']\n",
      " ['bustled']\n",
      " ['recrush']\n",
      " ['Radiolitidae']\n",
      " ['shellfish']\n",
      " ['reclimb']\n",
      " ['handmade']\n",
      " ['spacesaving']\n",
      " ['use']\n",
      " ['obversion']\n",
      " ['vermix']\n",
      " ['consenescency']\n",
      " ['artware']\n",
      " ['decadary']\n",
      " ['boyardom']\n",
      " ['platydactyl']\n",
      " ['xenophile']\n",
      " ['warwickite']\n",
      " ['melanize']\n",
      " ['uncognoscible']\n",
      " ['effective']\n",
      " ['transitivity']\n",
      " ['genially']\n",
      " ['nonexternality']\n",
      " ['unparticularized']\n",
      " ['Mesozoa']\n",
      " ['gyneconitis']\n",
      " ['tumulate']\n",
      " ['hemiramph']\n",
      " ['dratted']\n",
      " ['yaguaza']\n",
      " ['undernourishment']\n",
      " ['label']\n",
      " ['sialoid']\n",
      " ['nonglucosidal']\n",
      " ['curatial']\n",
      " ['countertype']\n",
      " ['bath']\n",
      " ['sprayboard']\n",
      " ['Boaedon']\n",
      " ['rascally']\n",
      " ['seege']\n",
      " ['firstling']\n",
      " ['cobwebbery']\n",
      " ['conkanee']\n",
      " ['vacuousness']\n",
      " ['coestablishment']\n",
      " ['reintrusion']\n",
      " ['cymbate']\n",
      " ['ravenwise']\n",
      " ['atta']\n",
      " ['pyelitic']\n",
      " ['constellatory']\n",
      " ['preobservation']\n",
      " ['gobernadora']\n",
      " ['hypocrital']\n",
      " ['alvearium']\n",
      " ['hierurgical']\n",
      " ['vagrantize']\n",
      " ['cavitied']\n",
      " ['sequently']\n",
      " ['scrutatory']\n",
      " ['slaughterously']\n",
      " ['axenic']\n",
      " ['grume']\n",
      " ['philanthropian']\n",
      " ['psychrophobia']\n",
      " ['Namaquan']\n",
      " ['apricate']\n",
      " ['traductionist']\n",
      " ['Zapus']\n",
      " ['discourageable']\n",
      " ['disputative']\n",
      " ['stockrider']\n",
      " ['recantation']\n",
      " ['paracymene']\n",
      " ['vividness']\n",
      " ['nonrationalized']\n",
      " ['prosternate']\n",
      " ['pope']\n",
      " ['egophonic']\n",
      " ['voluntaryism']\n",
      " ['perimedullary']\n",
      " ['physiognomics']\n",
      " ['paedotrophy']\n",
      " ['tarsoptosis']\n",
      " ['Pangasinan']\n",
      " ['sinuatoserrated']\n",
      " ['roughie']\n",
      " ['ureteroenteric']\n",
      " ['Kshatriyahood']\n",
      " ['centesimation']\n",
      " ['dishouse']\n",
      " ['cromaltite']\n",
      " ['Gasterotricha']\n",
      " ['lobe']\n",
      " ['photographer']\n",
      " ['suds']\n",
      " ['puli']\n",
      " ['empoison']\n",
      " ['flail']\n",
      " ['floscular']\n",
      " ['nasalize']\n",
      " ['satispassion']\n",
      " ['inopportune']\n",
      " ['bowlike']\n",
      " ['floristic']\n",
      " ['Satanist']\n",
      " ['disregarder']\n",
      " ['anesthetize']\n",
      " ['Cathay']\n",
      " ['hyperthetic']\n",
      " ['verdoy']\n",
      " ['cephalophyma']\n",
      " ['contrate']\n",
      " ['lociation']\n",
      " ['bikh']\n",
      " ['daytale']\n",
      " ['exceptional']\n",
      " ['lithoculture']\n",
      " ['illuminating']\n",
      " ['aller']\n",
      " ['sticker']\n",
      " ['dacoitage']\n",
      " ['redetermine']\n",
      " ['etymological']\n",
      " ['marker']\n",
      " ['remanagement']\n",
      " ['thickset']\n",
      " ['pseudocultural']\n",
      " ['rethunder']\n",
      " ['Jacobitical']\n",
      " ['interjealousy']\n",
      " ['alburn']\n",
      " ['edit']\n",
      " ['reducibility']\n",
      " ['microzoology']\n",
      " ['semiflashproof']\n",
      " ['tamably']\n",
      " ['traitorize']\n",
      " ['prestomial']\n",
      " ['sightfulness']\n",
      " ['pout']\n",
      " ['uncompleted']\n",
      " ['epinette']\n",
      " ['flexuous']\n",
      " ['alarming']\n",
      " ['cetane']\n",
      " ['feddan']\n",
      " ['elmy']\n",
      " ['resurround']\n",
      " ['notedly']\n",
      " ['magnecrystallic']\n",
      " ['pluriguttulate']\n",
      " ['Ingaevonic']\n",
      " ['washtray']\n",
      " ['fustianish']\n",
      " ['Mascouten']\n",
      " ['unicelled']\n",
      " ['impardonably']\n",
      " ['estimative']\n",
      " ['hydropically']\n",
      " ['condense']\n",
      " ['hypocrisy']\n",
      " ['hepatectomy']\n",
      " ['bibulousness']\n",
      " ['hematoporphyrinuria']\n",
      " ['omnisciently']\n",
      " ['shallowbrained']\n",
      " ['eidently']\n",
      " ['heteropodal']\n",
      " ['window']\n",
      " ['Philopteridae']\n",
      " ['cacotype']\n",
      " ['typotelegraphy']\n",
      " ['immorality']\n",
      " ['oblivionist']\n",
      " ['unquoted']\n",
      " ['papaya']\n",
      " ['hallage']\n",
      " ['quinolinyl']\n",
      " ['internee']\n",
      " ['ovariocele']\n",
      " ['Compositae']\n",
      " ['stereotype']\n",
      " ['acacin']\n",
      " ['amianthine']\n",
      " ['decadarch']\n",
      " ['unhandicapped']\n",
      " ['weeshy']\n",
      " ['unvulnerable']\n",
      " ['toxiferous']\n",
      " ['touristy']\n",
      " ['nonmodern']\n",
      " ['vaulty']\n",
      " ['palatable']\n",
      " ['zant']\n",
      " ['anthemia']\n",
      " ['zoa']\n",
      " ['zoogenous']\n",
      " ['conspirant']\n",
      " ['hoodoo']\n",
      " ['atocia']\n",
      " ['gudok']\n",
      " ['nitch']\n",
      " ['halberdman']\n",
      " ['Cochin']\n",
      " ['mintbush']\n",
      " ['mangonism']\n",
      " ['pickedly']\n",
      " ['Petunia']\n",
      " ['cosmic']\n",
      " ['refrangibleness']\n",
      " ['rescissible']\n",
      " ['anhungry']\n",
      " ['atelomitic']\n",
      " ['occupationalist']\n",
      " ['blindling']\n",
      " ['vestrify']\n",
      " ['resufferance']\n",
      " ['Lagerstroemia']\n",
      " ['unidentate']\n",
      " ['facetiousness']\n",
      " ['perpetual']\n",
      " ['unpaving']\n",
      " ['attargul']\n",
      " ['reconceal']\n",
      " ['discontented']\n",
      " ['reseize']\n",
      " ['Jur']\n",
      " ['unlatch']\n",
      " ['turkis']\n",
      " ['hardheadedness']\n",
      " ['eye']\n",
      " ['transude']\n",
      " ['pandrop']\n",
      " ['acromiodeltoid']\n",
      " ['transcending']\n",
      " ['scurrier']\n",
      " ['semiarchitectural']\n",
      " ['sclerencephalia']\n",
      " ['Sanvitalia']\n",
      " ['Jutic']\n",
      " ['astereognosis']\n",
      " ['millisecond']\n",
      " ['redesign']\n",
      " ['includedness']\n",
      " ['rogatory']\n",
      " ['unhelpfully']\n",
      " ['globularly']\n",
      " ['viddui']\n",
      " ['demeritoriously']\n",
      " ['unlegalness']\n",
      " ['herd']\n",
      " ['untucking']\n",
      " ['levoduction']\n",
      " ['polypaged']\n",
      " ['conicalness']\n",
      " ['squamule']\n",
      " ['catacromyodian']\n",
      " ['brachystochrone']\n",
      " ['entelodont']\n",
      " ['prolificalness']\n",
      " ['centibar']\n",
      " ['reliction']\n",
      " ['anthodium']\n",
      " ['pupilloscopy']\n",
      " ['indistinctiveness']\n",
      " ['telltalely']\n",
      " ['steenbok']\n",
      " ['unusuality']\n",
      " ['aureately']\n",
      " ['ignobility']\n",
      " ['hereinafter']\n",
      " ['Uchee']\n",
      " ['sorrento']\n",
      " ['artiad']\n",
      " ['Wasandawi']\n",
      " ['oligophrenia']\n",
      " ['gaufer']\n",
      " ['besour']\n",
      " ['connation']\n",
      " ['Ommiades']\n",
      " ['guardedly']\n",
      " ['flecnodal']\n",
      " ['Laplacian']\n",
      " ['gluepot']\n",
      " ['monandry']\n",
      " ['monopolize']\n",
      " ['inaccuracy']\n",
      " ['frondage']\n",
      " ['midland']\n",
      " ['renneting']\n",
      " ['monoethylamine']\n",
      " ['Vedanga']\n",
      " ['shamsheer']\n",
      " ['impoliteness']\n",
      " ['laicize']\n",
      " ['archsnob']\n",
      " ['Lepidoidei']\n",
      " ['acetbromamide']\n",
      " ['peritonsillar']\n",
      " ['encolumn']\n",
      " ['Wachaga']\n",
      " ['scissorbird']\n",
      " ['tornado']\n",
      " ['khalifa']\n",
      " ['nudity']\n",
      " ['cooser']\n",
      " ['bracteal']\n",
      " ['bloodspilling']\n",
      " ['declined']\n",
      " ['quadricarinate']\n",
      " ['scrupulousness']\n",
      " ['seel']\n",
      " ['semivolcanic']\n",
      " ['torques']\n",
      " ['fundament']\n",
      " ['pudgy']\n",
      " ['intreat']\n",
      " ['battlement']\n",
      " ['lomatine']\n",
      " ['carotinemia']\n",
      " ['lacis']\n",
      " ['guilelessness']\n",
      " ['misgrowth']\n",
      " ['coly']\n",
      " ['impenitibleness']\n",
      " ['anthropomorphism']\n",
      " ['flintworker']\n",
      " ['yok']\n",
      " ['polydaemoniac']\n",
      " ['intimate']\n",
      " ['multipotent']\n",
      " ['nonresidence']\n",
      " ['stonelike']\n",
      " ['reptilivorous']\n",
      " ['peasantship']\n",
      " ['Leptandra']\n",
      " ['subdolous']\n",
      " ['Isabelline']\n",
      " ['gateman']\n",
      " ['punga']\n",
      " ['clawker']\n",
      " ['formation']\n",
      " ['peseta']\n",
      " ['unidealism']\n",
      " ['retan']\n",
      " ['dauntingness']\n",
      " ['Monograptus']\n",
      " ['Limerick']\n",
      " ['plastics']\n",
      " ['unpolicied']\n",
      " ['lemon']\n",
      " ['enchannel']\n",
      " ['commation']\n",
      " ['fringilliform']\n",
      " ['hustlement']\n",
      " ['fusariose']\n",
      " ['sketchiness']\n",
      " ['garnetz']\n",
      " ['hogrophyte']\n",
      " ['Georgia']\n",
      " ['dorsothoracic']\n",
      " ['semipellucid']\n",
      " ['Epiphyllum']\n",
      " ['dietist']\n",
      " ['reincapable']\n",
      " ['azorite']\n",
      " ['Aberdeen']\n",
      " ['Zinzar']\n",
      " ['ambay']\n",
      " ['damped']\n",
      " ['espalier']\n",
      " ['oxygas']\n",
      " ['axiopisty']\n",
      " ['permutator']\n",
      " ['ipecacuanhic']\n",
      " ['airmonger']\n",
      " ['anisotrope']\n",
      " ['Christmasing']\n",
      " ['clubhaul']\n",
      " ['proteinous']\n",
      " ['duplicature']\n",
      " ['candlewaster']\n",
      " ['volucrine']\n",
      " ['wheezle']\n",
      " ['nonvortical']\n",
      " ['psychoanalytic']\n",
      " ['noncitizen']\n",
      " ['pontificalia']\n",
      " ['tegular']\n",
      " ['Alopecurus']\n",
      " ['Halimeda']\n",
      " ['pornerastic']\n",
      " ['tariff']\n",
      " ['anapeiratic']\n",
      " ['fructuary']\n",
      " ['synacmic']\n",
      " ['oxalaldehyde']\n",
      " ['puntabout']\n",
      " ['stela']\n",
      " ['clattertrap']\n",
      " ['bandaka']\n",
      " ['hyperequatorial']\n",
      " ['adenopharyngitis']\n",
      " ['scorpaenoid']\n",
      " ['superattainable']\n",
      " ['trachelitis']\n",
      " ['lettering']\n",
      " ['encomiastic']\n",
      " ['presbyterially']\n",
      " ['Tetragonia']\n",
      " ['planidorsate']\n",
      " ['spiculiferous']\n",
      " ['counterindentation']\n",
      " ['enwallow']\n",
      " ['suffixion']\n",
      " ['representative']\n",
      " ['virginal']\n",
      " ['Illyrian']\n",
      " ['stater']\n",
      " ['chuddar']\n",
      " ['iconism']\n",
      " ['besprinkler']\n",
      " ['ambiparous']\n",
      " ['pompon']\n",
      " ['commode']\n",
      " ['preantepenult']\n",
      " ['religious']\n",
      " ['Dichter']\n",
      " ['washiness']\n",
      " ['Bechtler']\n",
      " ['centricalness']\n",
      " ['unbrushed']\n",
      " ['suckhole']\n",
      " ['Fanti']\n",
      " ['breedable']\n",
      " ['deweylite']\n",
      " ['calorifacient']\n",
      " ['Zirak']\n",
      " ['unplumb']\n",
      " ['sting']\n",
      " ['unmarked']\n",
      " ['karamu']\n",
      " ['millboard']\n",
      " ['chick']\n",
      " ['pluridentate']\n",
      " ['monumentality']\n",
      " ['pneumatotherapeutics']\n",
      " ['Old']\n",
      " ['countermission']\n",
      " ['lettable']\n",
      " ['polyglycerol']\n",
      " ['euphonious']\n",
      " ['preaccumulate']\n",
      " ['unsailable']\n",
      " ['interzooecial']\n",
      " ['antihemorrheidal']\n",
      " ['uncaptiously']\n",
      " ['impregnant']\n",
      " ['aflicker']\n",
      " ['collinsite']\n",
      " ['suppletion']\n",
      " ['aposporous']\n",
      " ['persecutiveness']\n",
      " ['dialer']\n",
      " ['etherization']\n",
      " ['unidenticulate']\n",
      " ['agglomerated']\n",
      " ['affectional']\n",
      " ['marvelous']\n",
      " ['Chrysolophus']\n",
      " ['fructescent']\n",
      " ['rubedity']\n",
      " ['obliquate']\n",
      " ['unexcellent']\n",
      " ['Lorenzo']\n",
      " ['undissemblingly']\n",
      " ['directly']\n",
      " ['resink']\n",
      " ['pointel']\n",
      " ['njave']\n",
      " ['equivelocity']\n",
      " ['windlike']\n",
      " ['celestina']\n",
      " ['leucocytopoiesis']\n",
      " ['omphaloid']\n",
      " ['anastomotic']\n",
      " ['deteriorationist']\n",
      " ['coefficient']\n",
      " ['protomagnate']\n",
      " ['withholdal']\n",
      " ['presteel']\n",
      " ['obscurantic']\n",
      " ['gruffly']\n",
      " ['supertragic']\n",
      " ['radiative']\n",
      " ['Zolaist']\n",
      " ['uplifting']\n",
      " ['myocoelom']\n",
      " ['drugless']\n",
      " ['nontimbered']\n",
      " ['fabricant']\n",
      " ['diazoanhydride']\n",
      " ['undissuade']\n",
      " ['brushland']\n",
      " ['adenometritis']\n",
      " ['chichipe']\n",
      " ['Talmudistic']\n",
      " ['premillennialism']\n",
      " ['condylarth']\n",
      " ['micrococcal']\n",
      " ['awhirl']\n",
      " ['eerily']\n",
      " ['foreplanting']\n",
      " ['hawserwise']\n",
      " ['coercivity']\n",
      " ['retardation']\n",
      " ['reluctivity']\n",
      " ['unjudgelike']\n",
      " ['unadequateness']\n",
      " ['tractarian']\n",
      " ['overkind']\n",
      " ['strictly']\n",
      " ['equivocal']\n",
      " ['prespread']\n",
      " ['autobiographal']\n",
      " ['schizocoelous']\n",
      " ['pomade']\n",
      " ['ceride']\n",
      " ['balneotherapia']\n",
      " ['demandingly']\n",
      " ['aberrant']\n",
      " ['waiterlike']\n",
      " ['hydrothecal']\n",
      " ['preacidly']\n",
      " ['crystalliform']\n",
      " ['Neillia']\n",
      " ['barney']\n",
      " ['formamide']\n",
      " ['cyclopedical']\n",
      " ['rollicking']\n",
      " ['achiever']\n",
      " ['flatiron']\n",
      " ['dermatopathology']\n",
      " ['expiation']\n",
      " ['subsovereign']\n",
      " ['irritator']\n",
      " ['disparagingly']\n",
      " ['sleepland']\n",
      " ['efface']\n",
      " ['aftercooler']\n",
      " ['hackbush']\n",
      " ['withheld']\n",
      " ['aspersive']\n",
      " ['krobylos']\n",
      " ['mirthlessness']\n",
      " ['jirkinet']\n",
      " ['ergogram']\n",
      " ['Noemi']\n",
      " ['valetism']\n",
      " ['solenostele']\n",
      " ['unacquainted']\n",
      " ['mesem']\n",
      " ['icecraft']\n",
      " ['procacious']\n",
      " ['kamelaukion']\n",
      " ['petrosphere']\n",
      " ['spiderly']\n",
      " ['schemingly']\n",
      " ['atelostomia']\n",
      " ['corvoid']\n",
      " ['torturableness']\n",
      " ['catastrophe']\n",
      " ['scopulite']\n",
      " ['ambrosia']\n",
      " ['Taposa']\n",
      " ['encyclopedial']\n",
      " ['forswear']\n",
      " ['widow']\n",
      " ['pozzolanic']\n",
      " ['Rajput']\n",
      " ['mlechchha']\n",
      " ['inactively']\n",
      " ['promorph']\n",
      " ['cloacaline']\n",
      " ['retrogastric']\n",
      " ['oticodinia']\n",
      " ['sinner']\n",
      " ['pteridophytous']\n",
      " ['dirgeman']\n",
      " ['octactine']\n",
      " ['Wernerism']\n",
      " ['pollent']\n",
      " ['resistlessness']\n",
      " ['stolonate']\n",
      " ['Camassia']\n",
      " ['butylamine']\n",
      " ['interfamily']\n",
      " ['inundate']\n",
      " ['forcing']\n",
      " ['revulsively']\n",
      " ['rier']\n",
      " ['sloebush']\n",
      " ['numskulled']\n",
      " ['spermogenesis']\n",
      " ['forestful']\n",
      " ['friableness']\n",
      " ['liquefiable']\n",
      " ['opal']\n",
      " ['prestandardize']\n",
      " ['Paulinism']\n",
      " ['thermopleion']\n",
      " ['morrowing']\n",
      " ['enapt']\n",
      " ['juglandin']\n",
      " ['introgression']\n",
      " ['dochmiacal']\n",
      " ['placodermal']\n",
      " ['lintern']\n",
      " ['chipped']\n",
      " ['phytogenetically']\n",
      " ['missis']\n",
      " ['fa']\n",
      " ['rampacious']\n",
      " ['syconium']\n",
      " ['nonperpetuity']\n",
      " ['thysanopterous']\n",
      " ['indissolubility']\n",
      " ['notself']\n",
      " ['tronage']\n",
      " ['china']\n",
      " ['spiritualism']\n",
      " ['prehistorical']\n",
      " ['suithold']\n",
      " ['Cerambycidae']\n",
      " ['orthoptic']\n",
      " ['entrails']\n",
      " ['wreathe']\n",
      " ['biostatistics']\n",
      " ['Mousoni']\n",
      " ['preconclusion']\n",
      " ['infortunate']\n",
      " ['ineffectively']\n",
      " ['canso']\n",
      " ['Ranina']\n",
      " ['subadditive']\n",
      " ['misplay']\n",
      " ['sideage']\n",
      " ['overgrade']\n",
      " ['exceptive']\n",
      " ['inoffensively']\n",
      " ['shelterlessness']\n",
      " ['cathartic']\n",
      " ['buccinatory']\n",
      " ['cadastre']\n",
      " ['rearwardness']\n",
      " ['Maranta']\n",
      " ['dramatization']\n",
      " ['compressional']\n",
      " ['havercake']\n",
      " ['fungusy']\n",
      " ['ouphish']\n",
      " ['extruder']\n",
      " ['neoterist']\n",
      " ['toxiphobia']\n",
      " ['halcyonine']\n",
      " ['Lecanium']\n",
      " ['parasitotropism']\n",
      " ['underpinning']\n",
      " ['trichogen']\n",
      " ['forerunnership']\n",
      " ['lophophore']\n",
      " ['unclassable']\n",
      " ['aspectual']\n",
      " ['Ludolphian']\n",
      " ['kuttab']\n",
      " ['subtarget']\n",
      " ['dado']\n",
      " ['nasobronchial']\n",
      " ['nonstanzaic']\n",
      " ['subscribable']\n",
      " ['syringomyelia']\n",
      " ['hypophloeodic']\n",
      " ['almsdeed']\n",
      " ['Rua']\n",
      " ['blonde']\n",
      " ['amplexation']\n",
      " ['ozonous']\n",
      " ['isochasm']\n",
      " ['ginglyni']\n",
      " ['smouser']\n",
      " ['bromous']\n",
      " ['Elaphoglossum']\n",
      " ['gasometer']\n",
      " ['consilience']\n",
      " ['pallial']\n",
      " ['supercolossal']\n",
      " ['silentiary']\n",
      " ['sulfanilamide']\n",
      " ['longbeard']\n",
      " ['textually']\n",
      " ['larynx']\n",
      " ['hydropic']\n",
      " ['scoutish']\n",
      " ['truxilline']\n",
      " ['nonimperial']\n",
      " ['achromacyte']\n",
      " ['onomatoplasm']\n",
      " ['run']\n",
      " ['atropaceous']\n",
      " ['scend']\n",
      " ['dangerless']\n",
      " ['demyship']\n",
      " ['aimara']\n",
      " ['pseudodementia']\n",
      " ['meningomyelocele']\n",
      " ['Arcadianly']\n",
      " ['filmogen']\n",
      " ['narcissism']\n",
      " ['malahack']\n",
      " ['Latooka']\n",
      " ['unshifted']\n",
      " ['trichoschisis']\n",
      " ['sweeny']\n",
      " ['differentiable']\n",
      " ['abuse']\n",
      " ['thoraciform']\n",
      " ['schelling']\n",
      " ['discommendableness']\n",
      " ['bluebreast']\n",
      " ['ninthly']\n",
      " ['Salva']\n",
      " ['belderroot']\n",
      " ['papyrocracy']\n",
      " ['chirarthritis']\n",
      " ['adrenolytic']\n",
      " ['benzaminic']\n",
      " ['fancywork']\n",
      " ['intertransformability']\n",
      " ['cocamine']\n",
      " ['unpinked']\n",
      " ['Hetty']\n",
      " ['doggedly']\n",
      " ['unicycle']\n",
      " ['nonrationalist']\n",
      " ['besieging']\n",
      " ['hydatopneumatic']\n",
      " ['bloubiskop']\n",
      " ['courap']\n",
      " ['reschedule']\n",
      " ['orchioncus']\n",
      " ['orchialgia']\n",
      " ['dissociate']\n",
      " ['vert']\n",
      " ['upsetted']\n",
      " ['headborough']\n",
      " ['pachyglossate']\n",
      " ['redepreciate']\n",
      " ['rakehell']\n",
      " ['indispensability']\n",
      " ['Meibomian']\n",
      " ['stuntedness']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "import scipy.stats as stats\n",
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "word_list = random.sample(word_list,k=1000)\n",
    "word_vector = numpy.empty([1000,1],dtype = object)\n",
    "for i in range(1000):\n",
    "    word_vector[i][0] = word_list[i]    \n",
    "print(len(word_vector))\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter Fonts for Text Generation\n",
    "Next step is to choose which fonts will be used for text generation. There are many fonts out which replicate human writing styles. In this implementation 29 fonts are used. All the fonts are taken from Google Fonts, [https://fonts.google.com](url_here).<br>\n",
    "\n",
    "These are truetype fonts whose delegates are needed to be added to the library imagemagick used ahead in the implementation. Refer to any online resource or imagemagick's online forums for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "fonts = ['Amatic SC_700.ttf','Amatic SC_regular.ttf','Beth Ellen_regular.ttf','Caveat Brush_regular.ttf','Cedarville Cursive_regular.ttf','Coming Soon_regular.ttf','Dawning of a New Day_regular.ttf','Gaegu_300.ttf','Gaegu_700.ttf','Gaegu_regular.ttf','Give You Glory_regular.ttf','Indie Flower_regular.ttf','Just Me Again Down Here_regular.ttf','League Script_regular.ttf','Merienda_700.ttf','Merienda One_regular.ttf','Merienda_regular.ttf','Nanum Brush Script_regular.ttf','Nanum Pen Script_regular.ttf','Nothing You Could Do_regular.ttf','Over the Rainbow_regular.ttf','Reenie Beanie_regular.ttf','Sacramento_regular.ttf','Stalemate_regular.ttf','Sue Ellen Francisco_regular.ttf','Swanky and Moo Moo_regular.ttf','The Girl Next Door_regular.ttf','Waiting for the Sunrise_regular.ttf','Zeyada_regular.ttf'] \n",
    "print(len(fonts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage Methods Used\n",
    "The next task is to create single image generating functions which will further be used to create dataset of images.<br>\n",
    "In this implementation dataset is stored using 3 methods:<br>\n",
    "- Images in disk(.png format) and the metadata in csv\n",
    "- Images and metadata both in lmdb objects with images as list of numpy array and metadata as lists of string and numbers\n",
    "- Images and metadata in hdf5 format(used by Matlab as well). Images are stored as numpy array and metadata is stored as strings and numbers.\n",
    "\n",
    "Each method has its own advantages and disadvantages. lmdb and hdf5 are used because the bulk-data can be accessed in less time. Although these methods take large storage space as compared to storing on disk making them harder to download, there are efficient compression methods which reduces the storage size considerably especially for hdf5 format making it easy to download and upload.<br>\n",
    "\n",
    "The methods other than disk method has other advantages like consistency of data, data security as well. There are a lot of great resources to learn about these storage methods.\n",
    "\n",
    "# Libraries Used\n",
    "In this implementation various libraries are used but the major are the c-types binding of the imagemagick library for python named wand, PIL, pickle library to pickle and unpickle data lmdb and h5py for lmdb and hdf5 implementations. More can be read about installation and functionalities of these libraries on their official documentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating single image storage functions\n",
    "The image storage functions are implemented based on the algorithm gven in the paper: **Generating Synthetic Data for Text Recognition** by Praveen Krishnan and C.V. Jawahar.<br>\n",
    "In brief the points mentioned in the paper are as follows:\n",
    "-  The  vocabulary  of words needs tobe chosen from a dictionary.\n",
    "- For each word chosen, a random font is sampled\n",
    "- The image is rendered using the font rendered in the previous step.\n",
    "- In this process, following  parameters are varied according to some distribution:  \n",
    " - kerning  level  (inter  character  space),\n",
    " - strokewidth,  from  a  defined  distribution.\n",
    " - The  pixel  distribution  of both foreground and background pixels are sampled from the corresponding  pixels  for  both  regions  from  a  Gaussian  distribution  where  the parameters such as mean and standard deviation are learned from the Fg and Bg region of IAM dataset.\n",
    " - Gaussian filtering is done to smooth the rendered image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wand.color import Color\n",
    "from wand.image import Image\n",
    "from wand.drawing import Drawing, FontMetrics\n",
    "\n",
    "\n",
    "\n",
    "def datagen_diskfn(txt, fnt, fnt_sz, file_name, fmt = 'png'):\n",
    "        #The lowerlimits, upperlimits, mean and variances are derived from the IAM dataset\n",
    "        #as mentioned in the paper and are distributed through a gaussian distribution.\n",
    "        lowerlimit, upperlimit = 190, 255\n",
    "        mn, vari = 222.7, 8.45\n",
    "        A = stats.truncnorm((lowerlimit - mn) / vari, (upperlimit - mn) / vari, loc=mn, scale=vari)\n",
    "        d = A.rvs(1)\n",
    "        d0 = str(d[0])\n",
    "        \n",
    "        lowerlimitf, upperlimitf = 10, 60\n",
    "        mnf, varif = 33.34, 7.45\n",
    "        B = stats.truncnorm((lowerlimitf - mnf) / varif, (upperlimitf - mnf) / varif, loc=mnf, scale=varif)\n",
    "        c = B.rvs(1)\n",
    "        c0 = str(c[0])\n",
    "        \n",
    "        with Color('rgb('+c0+','+c0+','+c0+')') as fg:\n",
    "            with Color('rgb('+d0+','+d0+','+d0+')') as bg:\n",
    "                with Image(width=128, height=48, background = bg) as img:\n",
    "                    img.format = 'gray'\n",
    "                    img.type = 'grayscale'\n",
    "                    img.colorspace = 'gray'\n",
    "                    with Drawing() as draw:\n",
    "                        draw.font = fnt\n",
    "                        draw.font_size = fnt_sz\n",
    "                        draw.fill_color = fg\n",
    "                        lower, upper = -3.5, 5\n",
    "                        mu, sigma = 0, 1.25\n",
    "                        X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "                        a = X.rvs(1)\n",
    "                        draw.text_kerning = a[0]\n",
    "                        metrics = draw.get_font_metrics(img, txt)\n",
    "                        with Image(width=int(metrics.text_width)+5, height=int(metrics.text_height), background = bg) as im:\n",
    "                            img.format = 'gray'\n",
    "                            img.type = 'grayscale'\n",
    "                            img.colorspace = 'gray'\n",
    "                            draw.text(5,0+int(metrics.ascender)+1, txt)\n",
    "                            draw(im)\n",
    "                            im.resize(128,48)\n",
    "                            lowergb, uppergb = 0, 1.2\n",
    "                            mugb, sigmagb = 0.6, 0.2\n",
    "                            GB = stats.truncnorm((lowergb - mugb) / sigmagb, (uppergb - mugb) / sigmagb, loc=mugb, scale=sigmagb)\n",
    "                            gb = GB.rvs(1)\n",
    "                            im.gaussian_blur(0, gb[0])\n",
    "                            im.save(filename=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wand.display import display\n",
    "from wand.color import Color\n",
    "from wand.image import Image\n",
    "from wand.drawing import Drawing, FontMetrics\n",
    "import io\n",
    "import PIL\n",
    "\n",
    "def datagen_lmdbfn(txt, fnt, fnt_sz):\n",
    "    #The lowerlimits, upperlimits, mean and variances are derived from the IAM dataset\n",
    "        #as mentioned in the paper and are distributed through a gaussian distribution.\n",
    "        lowerlimit, upperlimit = 190, 255\n",
    "        mn, vari = 222.7, 8.45\n",
    "        A = stats.truncnorm((lowerlimit - mn) / vari, (upperlimit - mn) / vari, loc=mn, scale=vari)\n",
    "        d = A.rvs(1)\n",
    "        d0 = str(d[0])\n",
    "        \n",
    "        lowerlimitf, upperlimitf = 10, 60\n",
    "        mnf, varif = 33.34, 7.45\n",
    "        B = stats.truncnorm((lowerlimitf - mnf) / varif, (upperlimitf - mnf) / varif, loc=mnf, scale=varif)\n",
    "        c = B.rvs(1)\n",
    "        c0 = str(c[0])\n",
    "        \n",
    "        with Color('rgb('+c0+','+c0+','+c0+')') as fg:\n",
    "            with Color('rgb('+d0+','+d0+','+d0+')') as bg:\n",
    "                with Image(width=128, height=48, background = bg) as img:\n",
    "                    img.format = 'gray'\n",
    "                    img.type = 'grayscale'\n",
    "                    img.colorspace = 'gray'\n",
    "                    with Drawing() as draw:\t\n",
    "                        draw.font = fnt\n",
    "                        draw.font_size = fnt_sz\n",
    "                        draw.fill_color = fg\n",
    "                        lower, upper = -3.5, 5\n",
    "                        mu, sigma = 0, 1.25\n",
    "                        X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "                        a = X.rvs(1)\n",
    "                        draw.text_kerning = a[0]\n",
    "                        metrics = draw.get_font_metrics(img, txt)\n",
    "                        with Image(width=int(metrics.text_width)+5, height=int(metrics.text_height), background = bg) as im:\n",
    "                            im.format = 'gray'\n",
    "                            im.type = 'grayscale'\n",
    "                            im.colorspace = 'gray'\n",
    "                            draw.text(5,0+int(metrics.ascender)+1, txt)\n",
    "                            draw(im)\n",
    "                            im.resize(128,48)\n",
    "                            lowergb, uppergb = 0, 1.2\n",
    "                            mugb, sigmagb = 0.6, 0.2\n",
    "                            GB = stats.truncnorm((lowergb - mugb) / sigmagb, (uppergb - mugb) / sigmagb, loc=mugb, scale=sigmagb)\n",
    "                            gb = GB.rvs(1)\n",
    "                            im.gaussian_blur(0, gb[0])\n",
    "                            img_buffer = np.asarray(bytearray(im.make_blob(format='png')), dtype='uint8')\n",
    "                            bytesio = io.BytesIO(img_buffer)\n",
    "                            pil_img = PIL.Image.open(bytesio)\n",
    "                            q = np.array(pil_img)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_hdf5fn(txt, fnt, fnt_sz):\n",
    "        #The lowerlimits, upperlimits, mean and variances are derived from the IAM dataset\n",
    "        #as mentioned in the paper and are distributed through a gaussian distribution.\n",
    "        lowerlimit, upperlimit = 190, 255\n",
    "        mn, vari = 222.7, 8.45\n",
    "        A = stats.truncnorm((lowerlimit - mn) / vari, (upperlimit - mn) / vari, loc=mn, scale=vari)\n",
    "        d = A.rvs(1)\n",
    "        d0 = str(d[0])\n",
    "        \n",
    "        lowerlimitf, upperlimitf = 10, 60\n",
    "        mnf, varif = 33.34, 7.45\n",
    "        B = stats.truncnorm((lowerlimitf - mnf) / varif, (upperlimitf - mnf) / varif, loc=mnf, scale=varif)\n",
    "        c = B.rvs(1)\n",
    "        c0 = str(c[0])\n",
    "        \n",
    "        with Color('rgb('+c0+','+c0+','+c0+')') as fg:\n",
    "            with Color('rgb('+d0+','+d0+','+d0+')') as bg:\n",
    "                with Image(width=128, height=48, background = bg) as img:\n",
    "                    img.format = 'gray'\n",
    "                    img.type = 'grayscale'\n",
    "                    img.colorspace = 'gray'\n",
    "                    with Drawing() as draw:\t\n",
    "                        draw.font = fnt\n",
    "                        draw.font_size = fnt_sz\n",
    "                        draw.fill_color = fg\n",
    "                        lower, upper = -3.5, 5\n",
    "                        mu, sigma = 0, 1.25\n",
    "                        X = stats.truncnorm((lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "                        a = X.rvs(1)\n",
    "                        draw.text_kerning = a[0]\n",
    "                        metrics = draw.get_font_metrics(img, txt)\n",
    "                        with Image(width=int(metrics.text_width)+5, height=int(metrics.text_height), background = bg) as im:\n",
    "                            im.format = 'gray'\n",
    "                            im.type = 'grayscale'\n",
    "                            im.colorspace = 'gray'\n",
    "                            draw.text(5,0+int(metrics.ascender)+1, txt)\n",
    "                            draw(im)\n",
    "                            im.resize(128,48)\n",
    "                            lowergb, uppergb = 0, 1.2\n",
    "                            mugb, sigmagb = 0.6, 0.2\n",
    "                            GB = stats.truncnorm((lowergb - mugb) / sigmagb, (uppergb - mugb) / sigmagb, loc=mugb, scale=sigmagb)\n",
    "                            gb = GB.rvs(1)\n",
    "                            im.gaussian_blur(0, gb[0])\n",
    "                            img_buffer = np.asarray(bytearray(im.make_blob(format='png')), dtype='uint8')\n",
    "                            bytesio = io.BytesIO(img_buffer)\n",
    "                            pil_img = PIL.Image.open(bytesio)\n",
    "                            q = np.array(pil_img)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from pathlib import Path\n",
    "# from PIL import ImageDraw, ImageFont, Image\n",
    "# from PIL import ImageFilter\n",
    "# def datagen_diskfn(text, fnt, fnt_sz, file_name, fmt='PNG'):\n",
    "#     # create a font object \n",
    "#     font = ImageFont.truetype(fnt,fnt_sz)\n",
    "#     # determine dimensions of the text\n",
    "#     dim = font.getsize(text)\n",
    "#     print(dim)\n",
    "#     # create a new image slightly larger that the text\n",
    "#     im = Image.new('L', (dim[0]+2,dim[1]+2), 'grey')\n",
    "#     d = ImageDraw.Draw(im)\n",
    "#     x, y = im.size\n",
    "#     r = random.randint\n",
    "#     # add the text to the image\n",
    "#     d.text((0,-5.0), text,255, font=font)\n",
    "#     # save the image to a file\n",
    "#     im = im.resize((128,48))\n",
    "#     im.save(file_name, format = fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from pathlib import Path\n",
    "# from PIL import ImageDraw, ImageFont, Image\n",
    "# from PIL import ImageFilter\n",
    "# def datagen_lmdbfn(text, fnt, fnt_sz):\n",
    "#     # create a font object \n",
    "#     font = ImageFont.truetype(fnt,fnt_sz)\n",
    "#     # determine dimensions of the text\n",
    "#     dim = font.getsize(text)\n",
    "#     # create a new image slightly larger that the text\n",
    "#     im = Image.new('L', (dim[0]+2,dim[1]+2), 'grey')\n",
    "#     d = ImageDraw.Draw(im)\n",
    "#     x, y = im.size\n",
    "#     r = random.randint\n",
    "#     # add the text to the image\n",
    "#     d.text((0,-5), text,255, font=font)\n",
    "#     # save the image to a file\n",
    "#     #im.save(file_name, format=fmt)\n",
    "#     im = im.resize((128,48))\n",
    "#     im = np.array(im)\n",
    "#     return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# from pathlib import Path\n",
    "# from PIL import ImageDraw, ImageFont, Image\n",
    "# from PIL import ImageFilter\n",
    "# def datagen_hdf5fn(text, fnt, fnt_sz):\n",
    "#     # create a font object \n",
    "#     font = ImageFont.truetype(fnt,fnt_sz)\n",
    "#     # determine dimensions of the text\n",
    "#     dim = font.getsize(text)\n",
    "#     # create a new image slightly larger that the text\n",
    "#     im = Image.new('L', (dim[0]+2,dim[1]+2), 'grey')\n",
    "#     d = ImageDraw.Draw(im)\n",
    "#     x, y = im.size\n",
    "#     r = random.randint\n",
    "#     # add the text to the image\n",
    "#     d.text((0,-5), text,255, font=font)\n",
    "#     # save the image to a file\n",
    "#     #im.save(file_name, format=fmt)\n",
    "#     im = im.resize((128,48))\n",
    "#     im = np.array(im)\n",
    "#     return im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Object Definitions for lmdb objects\n",
    "To create lmdb image objects, a class is made which involves function for creating objects and a function for getting images back from the database. This class is necessary to be shipped with the data in order to reconstuct original images from the lmdb objects.<br>\n",
    "The object has following fields:<br>\n",
    "- channels - channels in the image\n",
    "- shape - dimensions of the image(in pixels)\n",
    "- image - byteform of the actual image\n",
    "- ALLlabel - label representing class id of each image and refers to list index in the ALLtext field\n",
    "- ALLtext - contains actual words\n",
    "- SET - contains 0 if image is in the validation set and 1 if image is in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageObject:\n",
    "    def __init__(self, image, label, text, trnORval):\n",
    "        # Dimensions of image for reconstruction - not really necessary \n",
    "        # for this dataset, but some datasets may include images of \n",
    "        # varying sizes\n",
    "        self.channels = 1\n",
    "        self.shape = image.shape[:2]\n",
    "        self.image = image.tobytes()\n",
    "        self.ALLlabel = label\n",
    "        self.ALLtext = text\n",
    "        self.SET = trnORval\n",
    "        \n",
    "    def get_image(self):\n",
    "        \"\"\" Returns the image as a numpy array. \"\"\"\n",
    "        image = np.frombuffer(self.image, dtype=np.uint8)\n",
    "        return image.reshape(*self.shape, self.channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage functions for the different methods\n",
    "## Disk Storage\n",
    "Contains images with following properties:\n",
    "- all images are stored in png format.\n",
    "- all images are scaled to (128X48) pixels size.\n",
    "- images are grayscale.\n",
    "\n",
    "Contains 5 files for metadata:<br>\n",
    "ALLlabel: label representing class id of each image and refers to list index in the ALLtext field<br>\n",
    "ALLtext: contains actual words<br>\n",
    "ALLnames: relative path of each image.<br>\n",
    "TRNInd: Values denoting image numbers which will be used for training<br>\n",
    "VALInd: Values denoting image numbers which will be used for validation<br>\n",
    "\n",
    "## lmdb Storage\n",
    "\n",
    "Contains images with following properties:\n",
    "- all images are stored in list of numpy array format.\n",
    "- all images are scaled to (128X48) pixels size.\n",
    "- images are grayscale.\n",
    "\n",
    "Contains 4 field in ImageObject for metadata:<br>\n",
    "multiimages: list of numpy array images to be stored.<br>\n",
    "ALLlabel: label representing class id of each image and refers to list index in the ALLtext field<br>\n",
    "ALLtext: contains actual words<br>\n",
    "TRNorVal: 0 or 1 denoting validation and training set respectively<br>\n",
    "\n",
    "## hdf5 Storage\n",
    "Contains images with following properties:\n",
    "- all images are stored in png format.\n",
    "- all images are scaled to (128X48) pixels size.\n",
    "- images are grayscale.\n",
    "\n",
    "Contains 5 files for metadata:<br>\n",
    "imageset: dataset of numpy array images to be stored h5py.h5t.NATIVE_UINT8 format<br>\n",
    "labelset: dataset of label representing class id of each image and refers to list index in the ALLtext field h5py.h5t.NATIVE_INT16 format<br>\n",
    "textset: dataset of contains actual words string format<br>\n",
    "trnset: dataset of Values denoting image numbers which will be used for training h5py.h5t.NATIVE_INT16 format<br>\n",
    "valset: dataset of Values denoting image numbers which will be used for validation h5py.h5t.NATIVE_INT16 format<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import lmdb\n",
    "import h5py\n",
    "import math\n",
    "def store_disk(num_images, num_fonts):\n",
    "    total_images = num_images*num_fonts\n",
    "    mylist = list(range(total_images))\n",
    "    random.shuffle(mylist)\n",
    "    val = mylist[:int(total_images/4)]\n",
    "    trn = mylist[int(total_images/4):]\n",
    "    val.sort()\n",
    "    trn.sort()\n",
    "    y_vector = numpy.empty([total_images,1],dtype = object)\n",
    "    y_list,name_list = [],[]\n",
    "    name_vector = numpy.empty([total_images,1],dtype = object)\n",
    "    \n",
    "   \n",
    "\n",
    "    for y in range(num_images):\n",
    "        new_dir = Path(\"datagen_disk/\"+str(y+1))\n",
    "        new_dir.mkdir(parents=True, exist_ok=True)\n",
    "        sample = random.sample(fonts,k=10)\n",
    "        for x in range (num_fonts):\n",
    "            datagen_diskfn(word_list[y], '/home/shreyansh/.fonts/typecatcher/'+str(sample[x]), 30, str(new_dir)+'/'+str(x)+'.png')\n",
    "            y_list.append(y)\n",
    "            path = str(new_dir)+'/'+str(x)+'.png'\n",
    "            name_list.append(path)\n",
    "            \n",
    "               \n",
    "    for i in range(total_images):\n",
    "        y_vector[i][0] = y_list[i]\n",
    "    \n",
    "    for i in range(total_images):\n",
    "        name_vector[i][0] = name_list[i]\n",
    "        \n",
    "    with open(\"datagen_disk/ALLtext.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(word_vector)\n",
    "    with open(\"datagen_disk/ALLlabels.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(y_vector)\n",
    "    with open(\"datagen_disk/ALLnames.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(name_vector)\n",
    "    with open(\"datagen_disk/VALInd.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(map(lambda x: [x], val))\n",
    "    with open(\"datagen_disk/TRNInd.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(map(lambda x: [x], trn))         \n",
    "                \n",
    "\n",
    "\n",
    "def store_lmdb(num_images, num_fonts):\n",
    "    total_images = num_images*num_fonts\n",
    "    mylist = list(range(total_images))\n",
    "    random.shuffle(mylist)\n",
    "    val = mylist[:int(total_images/4)]\n",
    "    \n",
    "    iterations = 0\n",
    "    map_size = total_images * 6144 * 10\n",
    "    multiimages,texts,labels,trnORval = [],[],[],[]\n",
    "    \n",
    "    for y in range(num_images):\n",
    "        sample = random.sample(fonts,k=10)\n",
    "        texts.append(word_list[y])\n",
    "        for x in range (num_fonts):\n",
    "            iterations += iterations\n",
    "            multiimages.append(datagen_lmdbfn(word_list[y], '/home/shreyansh/.fonts/typecatcher/'+str(sample[x]), 30))\n",
    "            labels.append(y)\n",
    "            if iterations in val:\n",
    "                trnORval.append(0)\n",
    "            else:\n",
    "                trnORval.append(1)\n",
    "            \n",
    "    \n",
    "\n",
    "    lmdb_dir = Path(\"datagen_lmdb\")\n",
    "    lmdb_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create a new LMDB DB for all the images\n",
    "    env = lmdb.open(str(lmdb_dir / f\"{total_images}_lmdb\"), map_size=map_size)\n",
    "\n",
    "    # Same as before  but let's write all the images in a single transaction\n",
    "    with env.begin(write=True) as txn:\n",
    "        j = -0.1\n",
    "        for i in range(total_images):\n",
    "            j = j + 0.1\n",
    "            k = math.floor(j)\n",
    "            # All key-value pairs need to be Strings\n",
    "            value = ImageObject(multiimages[i], labels[i],texts[k],trnORval[i])\n",
    "            key = f\"{i:08}\"\n",
    "            txn.put(key.encode(\"ascii\"), pickle.dumps(value))\n",
    "    env.close()\n",
    "\n",
    "def store_hdf5(num_images, num_fonts):\n",
    "    total_images = num_images*num_fonts\n",
    "    hdf5_dir = Path(\"datagen_hdf5\")\n",
    "    hdf5_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create a new HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{total_images}_many.h5\", \"w\")\n",
    "    \n",
    "    mylist = list(range(total_images))\n",
    "    random.shuffle(mylist)\n",
    "    listm = mylist[:int(total_images/4)]\n",
    "    \n",
    "    iterations = -1\n",
    "    multiimages,labels,trn, texts,val = [],[],[],[],[]\n",
    "    for y in range(num_images):\n",
    "        sample = random.sample(fonts,k=10)\n",
    "        texts.append(word_list[y])\n",
    "        \n",
    "        for x in range (num_fonts):\n",
    "            iterations +=1\n",
    "            multiimages.append(datagen_hdf5fn(word_list[y], '/home/shreyansh/.fonts/typecatcher/'+str(sample[x]), 30))\n",
    "            labels.append(y)\n",
    "            if iterations in listm:\n",
    "                val.append(iterations)\n",
    "            else:\n",
    "                trn.append(iterations)\n",
    "                \n",
    "            \n",
    "    asciiList = [n.encode(\"ascii\", \"ignore\") for n in texts]\n",
    "    \n",
    "    # Create a dataset in the file\n",
    "    dataset = file.create_dataset(\n",
    "        \"images\", np.shape(multiimages), h5py.h5t.NATIVE_UINT8, data=multiimages\n",
    "    )\n",
    "    labelset = file.create_dataset(\n",
    "        \"ALLlabels\", np.shape(labels), h5py.h5t.NATIVE_INT16, data=labels\n",
    "    )\n",
    "    textset = file.create_dataset(\n",
    "        \"ALLtext\", (len(asciiList),1), 'S100', data=asciiList\n",
    "    )\n",
    "    valset = file.create_dataset(\n",
    "        \"VALInd\", np.shape(val), h5py.h5t.NATIVE_INT16, data=val\n",
    "    )\n",
    "    trainset = file.create_dataset(\n",
    "        \"TRNInd\", np.shape(trn), h5py.h5t.NATIVE_INT16, data=trn\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: lmdb, Time usage: 85.25488042200232\n",
      "Method: hdf5, Time usage: 87.59862236800109\n",
      "Method: disk, Time usage: 85.13988823100226\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "num_images = 1000\n",
    "num_fonts = 10\n",
    "_store_many_funcs = dict(\n",
    "    disk=store_disk, lmdb=store_lmdb, hdf5=store_hdf5)\n",
    "\n",
    "from timeit import timeit\n",
    "\n",
    "store_many_timings = {\"disk\": [], \"lmdb\": [], \"hdf5\": []}\n",
    "\n",
    "for method in (\"lmdb\", \"hdf5\", \"disk\"):\n",
    "    t = timeit(\n",
    "        \"_store_many_funcs[method](num_images,num_fonts)\",\n",
    "        setup=\"\",\n",
    "        number=1,\n",
    "        globals=globals(),\n",
    "    )\n",
    "    store_many_timings[method].append(t)\n",
    "\n",
    "    # Print out the method, cutoff, and elapsed time\n",
    "    print(f\"Method: {method}, Time usage: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.stats._distn_infrastructure.rv_frozen'>\n",
      "[ 2.16062964  1.09557783 -2.439154    1.64288438  0.58234229  0.51439203\n",
      "  3.19627188  0.00965212  1.11780207 -0.63493254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyansh/anaconda3/envs/env1/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "lower, upper = -3.5, 6\n",
    "mu, sigma = 0.8, 1.5\n",
    "X = stats.truncnorm(\n",
    "    (lower - mu) / sigma, (upper - mu) / sigma, loc=mu, scale=sigma)\n",
    "N = stats.norm(loc=mu, scale=sigma)\n",
    "print(type(X))\n",
    "print(X.rvs(10))\n",
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "ax[0].hist(X.rvs(10000), normed=True)\n",
    "ax[1].hist(N.rvs(10000), normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
